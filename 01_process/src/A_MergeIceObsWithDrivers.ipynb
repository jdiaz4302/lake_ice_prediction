{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c41e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6486a583",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "\"Data release\" refers to https://www.sciencebase.gov/catalog/item/5e5c1b4fe4b01d50924f27e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dd3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_obs_fpath = \"../in/MN_ice/ice_duration_summarized.csv\"\n",
    "\n",
    "prior_data_release_dir = \"../in/prior_data_release/\"\n",
    "data_release_metadata_fpath = prior_data_release_dir + \"lake_metadata.csv\"\n",
    "\n",
    "out_dir = \"../out/\"\n",
    "out_driver_dir = out_dir + \"merged_drivers/by_DOW/\"\n",
    "matching_df_fpath = out_dir + \"matching_sources.csv\"\n",
    "missing_pgdl_fpath = out_dir + \"DOW_missing_PGDL_estimates.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5d0a7",
   "metadata": {},
   "source": [
    "# Import base data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254774bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_dur = pd.read_csv(ice_obs_fpath)\n",
    "data_release_metadata = pd.read_csv(data_release_metadata_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d857853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1156,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique DOWs exist in the ice targets data set?\n",
    "ice_dur['DOW'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808d61c",
   "metadata": {},
   "source": [
    "### Create a new data set for easily referencing between the observations and drivers\n",
    "\n",
    "The main usefulness here is that the ice observations are provided by Minnesota DOW identifiers while the TOHA data release drivers are primarily sorted by NHD identifiers. Sometimes one NHD identifier corresponds to multiple DOW identifiers, so this code unravels that duplication (i.e., individual rows rather than rows with list values) - being more repetitive but more easily accessible from the DOW point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ad3e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOW</th>\n",
       "      <th>group</th>\n",
       "      <th>meteo_file</th>\n",
       "      <th>nhdhr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3065700</td>\n",
       "      <td>06_N46.00-47.00_W94.50-97.00</td>\n",
       "      <td>nldas_meteo_N46.8125-46.8125_W96.1875-96.1875.csv</td>\n",
       "      <td>121545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6015200</td>\n",
       "      <td>09_N45.00-46.00_W94.50-97.00</td>\n",
       "      <td>nldas_meteo_N45.4375-45.4375_W96.5625-96.5625.csv</td>\n",
       "      <td>122548488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37004600</td>\n",
       "      <td>09_N45.00-46.00_W94.50-97.00</td>\n",
       "      <td>nldas_meteo_N45.0625-45.0625_W95.9375-95.9375.csv</td>\n",
       "      <td>122551004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37004601</td>\n",
       "      <td>09_N45.00-46.00_W94.50-97.00</td>\n",
       "      <td>nldas_meteo_N45.0625-45.0625_W95.9375-95.9375.csv</td>\n",
       "      <td>122551004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3029100</td>\n",
       "      <td>06_N46.00-47.00_W94.50-97.00</td>\n",
       "      <td>nldas_meteo_N46.9375-46.9375_W95.8125-95.8125.csv</td>\n",
       "      <td>121544299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>69027700</td>\n",
       "      <td>04_N45.50-48.00_W92.00-93.00</td>\n",
       "      <td>nldas_meteo_N47.8125-47.8125_W92.0625-92.0625.csv</td>\n",
       "      <td>105954667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>38062000</td>\n",
       "      <td>01_N48.00-49.50_W89.50-97.25</td>\n",
       "      <td>nldas_meteo_N48.0625-48.0625_W91.4375-91.4375.csv</td>\n",
       "      <td>80997051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>38014700</td>\n",
       "      <td>01_N48.00-49.50_W89.50-97.25</td>\n",
       "      <td>nldas_meteo_N48.0625-48.0625_W91.0625-91.0625.csv</td>\n",
       "      <td>80997393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>16081200</td>\n",
       "      <td>01_N48.00-49.50_W89.50-97.25</td>\n",
       "      <td>nldas_meteo_N48.0625-48.0625_W91.0625-91.0625.csv</td>\n",
       "      <td>80994457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>16072300</td>\n",
       "      <td>01_N48.00-49.50_W89.50-97.25</td>\n",
       "      <td>nldas_meteo_N48.0625-48.0625_W90.9375-90.9375.csv</td>\n",
       "      <td>80996367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1063 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           DOW                         group  \\\n",
       "0      3065700  06_N46.00-47.00_W94.50-97.00   \n",
       "1      6015200  09_N45.00-46.00_W94.50-97.00   \n",
       "2     37004600  09_N45.00-46.00_W94.50-97.00   \n",
       "3     37004601  09_N45.00-46.00_W94.50-97.00   \n",
       "4      3029100  06_N46.00-47.00_W94.50-97.00   \n",
       "...        ...                           ...   \n",
       "1058  69027700  04_N45.50-48.00_W92.00-93.00   \n",
       "1059  38062000  01_N48.00-49.50_W89.50-97.25   \n",
       "1060  38014700  01_N48.00-49.50_W89.50-97.25   \n",
       "1061  16081200  01_N48.00-49.50_W89.50-97.25   \n",
       "1062  16072300  01_N48.00-49.50_W89.50-97.25   \n",
       "\n",
       "                                             meteo_file      nhdhr  \n",
       "0     nldas_meteo_N46.8125-46.8125_W96.1875-96.1875.csv  121545300  \n",
       "1     nldas_meteo_N45.4375-45.4375_W96.5625-96.5625.csv  122548488  \n",
       "2     nldas_meteo_N45.0625-45.0625_W95.9375-95.9375.csv  122551004  \n",
       "3     nldas_meteo_N45.0625-45.0625_W95.9375-95.9375.csv  122551004  \n",
       "4     nldas_meteo_N46.9375-46.9375_W95.8125-95.8125.csv  121544299  \n",
       "...                                                 ...        ...  \n",
       "1058  nldas_meteo_N47.8125-47.8125_W92.0625-92.0625.csv  105954667  \n",
       "1059  nldas_meteo_N48.0625-48.0625_W91.4375-91.4375.csv   80997051  \n",
       "1060  nldas_meteo_N48.0625-48.0625_W91.0625-91.0625.csv   80997393  \n",
       "1061  nldas_meteo_N48.0625-48.0625_W91.0625-91.0625.csv   80994457  \n",
       "1062  nldas_meteo_N48.0625-48.0625_W90.9375-90.9375.csv   80996367  \n",
       "\n",
       "[1063 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for val in data_release_metadata['site_id']:\n",
    "    assert(val[:6] == 'nhdhr_') # check consistent format\n",
    "    remaining_val = val[6:]\n",
    "    assert(type(remaining_val) == str) # not list\n",
    "    \n",
    "DOW_nums_ls = []\n",
    "group_ls = []\n",
    "meteo_file_ls = []\n",
    "nhdhr_ls = []\n",
    "\n",
    "for DOW in data_release_metadata['mndow_id']:\n",
    "    DOW_nums = DOW.replace(\"|\", \"\").split('mndow_')[1:]\n",
    "    # unnesting the multiple lake aggregating and adding rows for them (and their files)\n",
    "    for ID in DOW_nums:\n",
    "        DOW_nums_ls.append(int(ID))\n",
    "        group_ls.append(data_release_metadata[data_release_metadata['mndow_id'] == DOW]['group_id'].item())\n",
    "        meteo_file_ls.append(data_release_metadata[data_release_metadata['mndow_id'] == DOW]['meteo_filename'].item())\n",
    "        nhdhr_ls.append(data_release_metadata[data_release_metadata['mndow_id'] == DOW]['site_id'].item()[6:])\n",
    "        \n",
    "matching_df = pd.DataFrame({'DOW':DOW_nums_ls,\n",
    "                            'group':group_ls,\n",
    "                            'meteo_file':meteo_file_ls,\n",
    "                            'nhdhr':nhdhr_ls})\n",
    "matching_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80d3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it for later use\n",
    "matching_df.to_csv(matching_df_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc528ab8",
   "metadata": {},
   "source": [
    "# For each DOW identifier, aggregrate and store all the data release drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e68f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner merge omits any lakes that do not have\n",
    "# corresponding values in the both data sources\n",
    "ice_dur = ice_dur.merge(matching_df,\n",
    "                        how = 'inner',\n",
    "                        on = 'DOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bfc1a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((633,), (581,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique DOWs and NHDHRs made it through the merge?\n",
    "ice_dur['DOW'].unique().shape, ice_dur['nhdhr'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a24531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_pgdl = []\n",
    "\n",
    "count = 0\n",
    "for DOW in ice_dur['DOW'].unique():\n",
    "    \n",
    "    # subset the df for value grabbing\n",
    "    cur_df = ice_dur[ice_dur['DOW'] == DOW].reset_index(drop=True)\n",
    "    \n",
    "    group = cur_df['group'].unique().item()\n",
    "    meteo_file = cur_df['meteo_file'].unique().item()\n",
    "    nhdhr = cur_df['nhdhr'].unique().item()\n",
    "    \n",
    "    # get the meteo inputs\n",
    "    meteo = pd.read_csv(prior_data_release_dir +\n",
    "                        'inputs_' +\n",
    "                        group +\n",
    "                        '/' +\n",
    "                        meteo_file)\n",
    "\n",
    "    # get the clarity inputs\n",
    "    clarity = pd.read_csv(prior_data_release_dir +\n",
    "                          'clarity_' +\n",
    "                          group +\n",
    "                          '/gam_nhdhr_' +\n",
    "                          nhdhr + \n",
    "                          '_clarity.csv') \n",
    "\n",
    "    # get the PB ice flags\n",
    "    ice_flags = pd.read_csv(prior_data_release_dir +\n",
    "                            'ice_flags_' +\n",
    "                            group +\n",
    "                            '/pb0_nhdhr_' +\n",
    "                            nhdhr + \n",
    "                            '_ice_flags.csv') \n",
    "\n",
    "    # get the PB irradiance\n",
    "    irradiance = pd.read_csv(prior_data_release_dir +\n",
    "                             'irradiance_' +\n",
    "                             group +\n",
    "                             '/pb0_nhdhr_' +\n",
    "                             nhdhr + \n",
    "                             '_irradiance.csv')   \n",
    "\n",
    "    # get the PB water temps\n",
    "    PB_water_temps = pd.read_csv(prior_data_release_dir +\n",
    "                                 'pb0_predictions_' +\n",
    "                                 group +\n",
    "                                 '/pb0_nhdhr_' +\n",
    "                                 nhdhr + \n",
    "                                 '_temperatures.csv')\n",
    "\n",
    "    # get the PGDL water temps WHEN THEY ARE AVAILABLE\n",
    "    try:\n",
    "        PGDL_water_temps = pd.read_csv(prior_data_release_dir +\n",
    "                                       'pgdl_predictions_' +\n",
    "                                       group + \n",
    "                                       '/tmp/pgdl_nhdhr_' +\n",
    "                                       nhdhr + \n",
    "                                       '_temperatures.csv')\n",
    "    # otherwise proceed without them\n",
    "    except FileNotFoundError as e:\n",
    "        missing_pgdl.append(DOW)\n",
    "        PGDL_water_temps = None\n",
    "\n",
    "    # merge them all\n",
    "    meteo = meteo.rename(columns = {'time':'date'})\n",
    "    inputs = meteo.merge(clarity, on = 'date', how = 'left')\n",
    "    inputs = inputs.merge(ice_flags, on = 'date', how = 'left')\n",
    "    inputs = inputs.merge(irradiance, on = 'date', how = 'left')\n",
    "    inputs = inputs.merge(PB_water_temps, on = 'date', how = 'left')\n",
    "    try:\n",
    "        inputs = inputs.merge(PGDL_water_temps, on = 'date', how = 'left')\n",
    "    except TypeError as e:\n",
    "        pass\n",
    "\n",
    "    inputs.to_csv(out_driver_dir + \"DOW_\" + str(DOW) + \"_all_vars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea1c1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many lakes (that made it through the merge) lacked PGDL estimates?\n",
    "len(missing_pgdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2044d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(missing_pgdl_fpath, missing_pgdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8265b5",
   "metadata": {},
   "source": [
    "Originally, I merged these by nhdhr, but I ultimately decided on DOW as the identifier because DOW is what we have labels for, so it seems more important to be faithful to the labels than the drivers (which are by nhdhr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844c00b",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "# Some Quality Checks\n",
    "\n",
    "Mostly concerned with PGDL estimates\n",
    "\n",
    "### Quality checking where we have missing data\n",
    "\n",
    "Identify all the unique lakes and count how many ice observations each unique lake has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0619d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ls = []\n",
    "for DOW in ice_dur['DOW'].unique():\n",
    "    count_ls.append(ice_dur[ice_dur['DOW'] == DOW]['DOW'].value_counts().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c7e438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({'DOW':ice_dur['DOW'].unique(),\n",
    "                       'count':count_ls})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a83e02",
   "metadata": {},
   "source": [
    "Now, match that information with which unique lakes are lacking pgdl driver data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb523641",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(new_df.shape[0]):\n",
    "    indices.append(new_df['DOW'][i] in missing_pgdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a68b2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[indices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d73f336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(975, 0.19102664576802508)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(new_df['count'][indices]), np.sum(new_df['count'][indices]) / ice_dur.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ccc181",
   "metadata": {},
   "source": [
    "The unique lakes that lack any PGDL estimates amount to 975 observations, which is 19% of the data set. This pays no attention to the timing of PGDL estimates or observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a2178",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "### Quality checking when we have missing data\n",
    "\n",
    "Same procedure as above\n",
    "\n",
    "BUT, first remove years that don't have any driver data anyways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b577b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_years = ice_dur[(ice_dur['min_ice_on_date'] > '1980-01-01') * (ice_dur['min_ice_on_date'] <= '2018-07-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "179c17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ls = []\n",
    "for DOW in relevant_years['DOW'].unique():\n",
    "    count_ls.append(relevant_years[relevant_years['DOW'] == DOW]['DOW'].value_counts().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15b8b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({'DOW':relevant_years['DOW'].unique(),\n",
    "                       'count':count_ls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59d73dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4359, 0.1459639498432602)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(new_df['count']), (ice_dur.shape[0] - np.sum(new_df['count'])) / ice_dur.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de04983",
   "metadata": {},
   "source": [
    "Years that are outside the temporal bounds of the driver data represent approximately 15% of the data set alone.\n",
    "\n",
    "Beyond that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33146a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(new_df.shape[0]):\n",
    "    indices.append(new_df['DOW'][i] in missing_pgdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2a868f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(873, 0.2002752924982794)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what?? it should be 15%\n",
    "# how can 15% of sequences have non-NA pgdl but 20% of them are missing files\n",
    "np.sum(new_df['count'][indices]), np.sum(new_df['count'][indices]) / np.sum(new_df['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d02d4a",
   "metadata": {},
   "source": [
    "An additional ~20% lack PGDL estimates due to their lakes not having PGDL estimates for any time period.\n",
    "\n",
    "So ~35% of observations do not have PGDL estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45f6da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
