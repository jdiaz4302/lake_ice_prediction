{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bbacd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/software/common/arc/apps/jupyter/conda/envs/jupyter3.4/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed377b8a",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb3c23",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79e8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_out_dir = '01_process/out/'\n",
    "\n",
    "train_data_fpath = process_out_dir + 'train_data.npz'\n",
    "valid_data_fpath = process_out_dir + 'valid_data.npz'\n",
    "# not doing any test set stuff until the very, very end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdc6456-9d25-4563-b333-4b9e12e0da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_dir = '/caldera/projects/usgs/water/iidd/datasci/lake-temp/lake_ice_prediction/'\n",
    "\n",
    "process_out_dir = extended_dir + process_out_dir\n",
    "\n",
    "train_data_fpath = extended_dir + train_data_fpath\n",
    "valid_data_fpath = extended_dir + valid_data_fpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0168d",
   "metadata": {},
   "source": [
    "### Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9bf17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000 is same epochs as first LSTM\n",
    "epochs = 10000\n",
    "# different, coarser printing compared to other models that\n",
    "# early stop much sooner\n",
    "coarse_epoch_printing = 50\n",
    "\n",
    "# model hyperparams\n",
    "random_seed = 4 # change for different 'random' initializations\n",
    "model_dim = 512\n",
    "ff_dim = 2048\n",
    "nheads = 8\n",
    "n_enc_layers = 6\n",
    "\n",
    "# data loader hyperparams\n",
    "bs = 100\n",
    "shuffle = True\n",
    "pin_memory = True # supposedly faster for CPU->GPU transfers\n",
    "\n",
    "# training hyperparams\n",
    "early_stop_patience = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f577fe",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20cd188-5c07-41e7-aaf4-4e10f0703421",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out_dir = '02_train/out/'\n",
    "\n",
    "# note that file names are adjusted with seed value\n",
    "data_scalars_fpath =  train_out_dir + 'massive_transformer_min_max_scalars_' + str(random_seed) + '_.pt'\n",
    "model_weights_fpath = train_out_dir + 'massive_transformer_weights_' + str(random_seed) + '_.pth'\n",
    "train_predictions_fpath = train_out_dir + 'massive_transformer_train_preds_' + str(random_seed) + '_.npy'\n",
    "valid_predictions_fpath = train_out_dir + 'massive_transformer_valid_preds_' + str(random_seed) + '_.npy'\n",
    "loss_lists_fpath = train_out_dir + 'massive_transformer_loss_lists_' + str(random_seed) + '_.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "908a8239-ada5-4d92-8882-768016c747fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scalars_fpath = extended_dir + data_scalars_fpath\n",
    "model_weights_fpath = extended_dir + model_weights_fpath\n",
    "train_predictions_fpath = extended_dir + train_predictions_fpath\n",
    "valid_predictions_fpath = extended_dir + valid_predictions_fpath\n",
    "loss_lists_fpath = extended_dir + loss_lists_fpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36116278",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8888eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(train_data_fpath, allow_pickle = True)\n",
    "valid_data = np.load(valid_data_fpath, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a617f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data['x']\n",
    "train_y = train_data['y']\n",
    "train_dates = train_data['dates']\n",
    "train_DOW = train_data['DOW']\n",
    "train_variables = train_data['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68494e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = valid_data['x']\n",
    "valid_y = valid_data['y']\n",
    "valid_dates = valid_data['dates']\n",
    "valid_DOW = valid_data['DOW']\n",
    "valid_variables = valid_data['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61388316",
   "metadata": {},
   "source": [
    "# Prepare data for `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7708b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = torch.from_numpy(train_y).float().unsqueeze(2) # adding a feature dimension to Ys\n",
    "train_x = torch.from_numpy(train_x).float()\n",
    "\n",
    "valid_y = torch.from_numpy(valid_y).float().unsqueeze(2)\n",
    "valid_x = torch.from_numpy(valid_x).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918e72d",
   "metadata": {},
   "source": [
    "# min-max scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11139b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scalars = torch.zeros(train_x.shape[2], 2)\n",
    "\n",
    "for i in range(train_x.shape[2]):\n",
    "    min_max_scalars[i, 0] = train_x[:, :, i].min()\n",
    "    min_max_scalars[i, 1] = train_x[:, :, i].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e846557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_x.shape[2]):\n",
    "    # scale train set with train min/max\n",
    "    train_x[:, :, i] = ((train_x[:, :, i] - min_max_scalars[i, 0]) /\n",
    "                        (min_max_scalars[i, 1] - min_max_scalars[i, 0]))\n",
    "    # scale valid set with train min/max\n",
    "    valid_x[:, :, i] = ((valid_x[:, :, i] - min_max_scalars[i, 0]) /\n",
    "                        (min_max_scalars[i, 1] - min_max_scalars[i, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2924bb9e",
   "metadata": {},
   "source": [
    "# Define a simple transformer (encoder-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9165caf7-1cf2-42c6-9d55-cd73cda86465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 365):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe = torch.moveaxis(pe, [0, 1, 2], [1, 0, 2]) # BATCH FIRST\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa05292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recycled model code\n",
    "class BasicTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, model_dim, nheads, ff_dim, n_encoder_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, model_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(model_dim)\n",
    "        \n",
    "        self.mask = (torch.triu(torch.ones((seq_len, seq_len))) == 0)\n",
    "        self.mask = self.mask.transpose(0, 1)\n",
    "        self.mask = self.mask.cuda()\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model = model_dim,\n",
    "                                                    nhead = nheads,\n",
    "                                                    batch_first = True,\n",
    "                                                    dim_feedforward = ff_dim)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layers, n_encoder_layers)\n",
    "        \n",
    "        self.dense = nn.Linear(model_dim, 1)\n",
    "        self.dense_activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        \n",
    "        embed = self.embedding(x)\n",
    "        positioned_embed = self.pos_encoding(embed)\n",
    "        # mask = src_mask for nn.TransformerEncoder\n",
    "        encoded = self.encoder(positioned_embed, mask = self.mask)\n",
    "        out = self.dense_activation(self.dense(encoded))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "741d5f3e-58a2-49fc-8a76-23381a437359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fbb8cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18920961"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the model with a seed\n",
    "torch.manual_seed(random_seed)\n",
    "# initialize random mini batches with numpy seed\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "model = BasicTransformer(train_x.shape[2], train_x.shape[1], model_dim, nheads, ff_dim, n_enc_layers).cuda()\n",
    "\n",
    "# print number of model params\n",
    "sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58421b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_ls = []\n",
    "valid_loss_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d9c595f-6319-47e6-b7c9-d9cdf89a914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "for i in range(len(train_x)):\n",
    "    train_dataset.append([train_x[i], train_y[i]])\n",
    "    \n",
    "valid_dataset = []\n",
    "for i in range(len(valid_x)):\n",
    "    valid_dataset.append([valid_x[i], valid_y[i]])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=shuffle, pin_memory=pin_memory)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=bs, shuffle=shuffle, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bcee350-be17-4611-8f68-72a62b4b58d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6644801497459412 0.6771087334426817 0.6771087334426817\n",
      "50 0.5309036374092102 0.5237719137582753 0.5181432453879359\n",
      "100 0.458225280046463 0.46543416777145824 0.45245703254049835\n",
      "150 0.45827680826187134 0.4599741347278584 0.43024161903811947\n",
      "200 0.43358466029167175 0.4400907417248491 0.429577471311733\n",
      "250 0.44539788365364075 0.4593915569485059 0.41843229078189814\n",
      "300 0.4675615429878235 0.41380347728399025 0.40698383323373555\n",
      "350 0.4190325140953064 0.43362935733597036 0.40346955493546593\n",
      "381 0.42458941539128625 0.4119205119041855 0.40346955493546593\n",
      "CPU times: user 53min 27s, sys: 56min 24s, total: 1h 49min 52s\n",
      "Wall time: 1h 16min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "worsened_valid_loss_count = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    # for each epoch, perform multiple training updates with the random mini batches of the whole training set\n",
    "    cur_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Move data to GPU\n",
    "        batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "        # Predict and compute loss\n",
    "        batch_y_hat = model(batch_x)\n",
    "        batch_loss = loss_fn(batch_y_hat, batch_y)\n",
    "        # Improve model\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Track set-wide loss\n",
    "        cur_loss += batch_loss.item() * batch_x.shape[0]/train_x.shape[0]\n",
    "        \n",
    "    # Likewise, generate predictions for the entire validation set\n",
    "    cur_val_loss = 0\n",
    "    # No gradient to maximize hardware use (and not needed for validation)\n",
    "    with torch.no_grad():\n",
    "        # For all random mini batches in the validation set...\n",
    "        for batch_val_x, batch_val_y in valid_loader:\n",
    "            # Again, move to GPU\n",
    "            batch_val_x, batch_val_y = batch_val_x.cuda(), batch_val_y.cuda()\n",
    "            # Predict and eval loss\n",
    "            batch_val_y_hat = model(batch_val_x)\n",
    "            batch_val_loss = loss_fn(batch_val_y_hat, batch_val_y)\n",
    "            # Track set-wide loss\n",
    "            cur_val_loss += batch_val_loss.item() * batch_val_x.shape[0]/valid_x.shape[0]\n",
    "    \n",
    "    # Store new set-wide losses\n",
    "    loss_ls.append(cur_loss)\n",
    "    valid_loss_ls.append(cur_val_loss)\n",
    "    \n",
    "    # Early stopping: determine if validation set performance is degrading\n",
    "    if cur_val_loss > min(valid_loss_ls):\n",
    "        worsened_valid_loss_count += 1\n",
    "        # Break after our patience has been exhausted\n",
    "        if worsened_valid_loss_count == early_stop_patience:\n",
    "            break\n",
    "    # Only save model weights if validation set performance is improving\n",
    "    else:\n",
    "        worsened_valid_loss_count = 0\n",
    "        torch.save(model.state_dict(), model_weights_fpath)\n",
    "        \n",
    "    # Occasionally print the current state\n",
    "    if i % coarse_epoch_printing == 0:\n",
    "        # epoch, current train loss, current valid loss, best valid loss\n",
    "        print(i, batch_loss.item(), cur_val_loss, min(valid_loss_ls))\n",
    "        \n",
    "# Final print of the current state\n",
    "print(i, cur_loss, cur_val_loss, min(valid_loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73619b32-4902-43ef-a4ed-f8b33cc9801b",
   "metadata": {},
   "source": [
    "# Save stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb3e27ef-de8a-4f74-acaf-4262314556ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = None\n",
    "batch_x = None\n",
    "batch_y = None\n",
    "batch_y_hat = None\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d768a9b-ffbb-465a-bd36-154dffa2a876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicTransformer(\n",
       "  (embedding): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dense_activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the best weights and stop performing dropout\n",
    "model.load_state_dict(torch.load(model_weights_fpath))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e424b678-0cac-4f14-a540-5d29cd0bc79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using the data loader is simpler for variable Shuffle=True/False\n",
    "# (and I implemented this prior to using formal data loaders)\n",
    "def generate_all_preds_via_batch(x_tensor, batch_size):\n",
    "    # make empty array for predictions\n",
    "    y_hat_tensor = torch.zeros([x_tensor.shape[0], x_tensor.shape[1], 1])\n",
    "    \n",
    "    # until we use all the possible sequential batches...\n",
    "    count = 1\n",
    "    loop_max = int(np.ceil(x_tensor.shape[0] / batch_size))\n",
    "    for i in range(loop_max):\n",
    "        min_i = (count-1)*bs\n",
    "        max_i = count*bs\n",
    "        # generate batch-sized predictions\n",
    "        if i != (loop_max - 1):\n",
    "            with torch.no_grad():\n",
    "                y_hat_tensor[min_i:max_i] = model(x_tensor[min_i:max_i].cuda()).cpu()\n",
    "        # or remaining-sized predictions\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                y_hat_tensor[min_i:] = model(x_tensor[min_i:].cuda()).cpu()\n",
    "        # update batch count\n",
    "        count += 1\n",
    "        \n",
    "    return y_hat_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac34f714-2754-447c-8e79-f1a463a648b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_hat = generate_all_preds_via_batch(train_x, bs)\n",
    "valid_y_hat = generate_all_preds_via_batch(valid_x, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa0d30f3-e015-45d2-880d-ce901f652905",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_hat = train_y_hat.numpy()\n",
    "valid_y_hat = valid_y_hat.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8ef5644-06cd-47e6-b640-82ea33368dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(train_predictions_fpath, train_y_hat)\n",
    "np.save(valid_predictions_fpath, valid_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58fcc0a0-8d9a-44bc-a5ba-34bebda7355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(min_max_scalars, data_scalars_fpath)\n",
    "torch.save(model.state_dict(), model_weights_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5340745-12c7-4dc6-916f-5d80f6a00e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train_loss':loss_ls, 'valid_loss':valid_loss_ls}\n",
    "np.savez_compressed(loss_lists_fpath, **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700afe24-ad6f-4257-bb30-5430d5cc6fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
